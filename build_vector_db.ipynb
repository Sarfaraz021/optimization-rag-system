{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A: Data Ingestion & Vector Database\n",
    "Build vector database from cloud cost optimization sources using OpenAI embeddings and PGVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_postgres import PGVector\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ Imports loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Using embedding model: text-embedding-3-small\n",
      "üíæ Collection name: cloud_cost_optimization\n"
     ]
    }
   ],
   "source": [
    "# OpenAI Configuration\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "\n",
    "# PGVector Configuration\n",
    "CONNECTION = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"\n",
    "COLLECTION_NAME = \"cloud_cost_optimization\"\n",
    "\n",
    "# Chunking Configuration\n",
    "CHUNK_SIZE = 400  # tokens (roughly 1600 characters)\n",
    "CHUNK_OVERLAP = 50  # tokens\n",
    "\n",
    "print(f\"üìù Using embedding model: {EMBEDDING_MODEL}\")\n",
    "print(f\"üíæ Collection name: {COLLECTION_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Configured 23 data sources\n",
      "  - AWS Cost Optimization Pillar (AWS)\n",
      "  - AWS S3 Storage Classes (AWS)\n",
      "  - AWS EC2 Pricing (AWS)\n",
      "  - AWS Cost Optimization Blog (AWS)\n",
      "  - Azure Cost Management Best Practices (Azure)\n",
      "  - Azure Advisor Cost Recommendations (Azure)\n",
      "  - Azure Storage Cost Optimization (Azure)\n",
      "  - GCP Cost Optimization Best Practices (GCP)\n",
      "  - GCP Cloud Storage Cost Optimization (GCP)\n",
      "  - GCP Compute Engine Pricing (GCP)\n",
      "  - AWS Reserved Instances Guide (AWS)\n",
      "  - AWS Spot Instances Best Practices (AWS)\n",
      "  - Azure Reserved VM Instances (Azure)\n",
      "  - Azure Spot Virtual Machines (Azure)\n",
      "  - GCP Committed Use Discounts (GCP)\n",
      "  - GCP Preemptible VM Instances (GCP)\n",
      "  - AWS Lambda Cost Optimization (AWS)\n",
      "  - Azure Functions Cost Optimization (Azure)\n",
      "  - GCP Cloud Functions Pricing (GCP)\n",
      "  - AWS RDS Cost Optimization (AWS)\n",
      "  - Azure SQL Database Cost Optimization (Azure)\n",
      "  - GCP Cloud SQL Cost Optimization (GCP)\n",
      "  - Cloud Cost Optimization Guide - Cloudability (Third-party)\n"
     ]
    }
   ],
   "source": [
    "# Load sources from YAML configuration for better management\n",
    "import yaml\n",
    "\n",
    "with open('data/sources.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    SOURCES = config['sources']\n",
    "\n",
    "print(f\"üìö Configured {len(SOURCES)} data sources\")\n",
    "for source in SOURCES:\n",
    "    print(f\"  - {source['name']} ({source['provider']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Scrape and Clean Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping sources:   0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped AWS Cost Optimization Pillar: 2659 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping sources:   4%|‚ñç         | 1/23 [00:07<02:54,  7.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped AWS S3 Storage Classes: 27231 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping sources:   9%|‚ñä         | 2/23 [00:11<01:51,  5.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped AWS EC2 Pricing: 3109 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping sources:  13%|‚ñà‚ñé        | 3/23 [00:15<01:34,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped AWS Cost Optimization Blog: 4265 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping sources:  17%|‚ñà‚ñã        | 4/23 [00:17<01:09,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped Azure Cost Management Best Practices: 19508 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping sources:  22%|‚ñà‚ñà‚ñè       | 5/23 [00:21<01:06,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped Azure Advisor Cost Recommendations: 10547 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping sources:  26%|‚ñà‚ñà‚ñå       | 6/23 [00:25<01:05,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped Azure Storage Cost Optimization: 22145 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping sources:  30%|‚ñà‚ñà‚ñà       | 7/23 [00:29<01:03,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped GCP Cost Optimization Best Practices: 16994 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping sources:  35%|‚ñà‚ñà‚ñà‚ñç      | 8/23 [00:37<01:20,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped GCP Cloud Storage Cost Optimization: 9818 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping sources:  39%|‚ñà‚ñà‚ñà‚ñâ      | 9/23 [00:40<01:02,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped GCP Compute Engine Pricing: 100716 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping sources:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 10/23 [00:46<01:06,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped AWS Reserved Instances Guide: 6876 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping sources:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 11/23 [00:50<00:54,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped AWS Spot Instances Best Practices: 14332 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping sources:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 12/23 [00:52<00:43,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped Azure Reserved VM Instances: 11933 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping sources:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 13/23 [00:55<00:36,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped Azure Spot Virtual Machines: 9817 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping sources:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 14/23 [00:59<00:32,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped GCP Committed Use Discounts: 32038 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping sources:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 15/23 [01:02<00:28,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped GCP Preemptible VM Instances: 11282 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping sources:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 16/23 [01:08<00:28,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped AWS Lambda Cost Optimization: 29618 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping sources:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 17/23 [01:13<00:26,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped Azure Functions Cost Optimization: 21065 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping sources:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 18/23 [01:16<00:20,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped GCP Cloud Functions Pricing: 685 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping sources:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 19/23 [01:20<00:15,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped AWS RDS Cost Optimization: 35746 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping sources:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 20/23 [01:22<00:10,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped Azure SQL Database Cost Optimization: 11199 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping sources:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 21/23 [01:26<00:07,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping https://cloud.google.com/sql/docs/mysql/optimize-costs: 404 Client Error: Not Found for url: https://cloud.google.com/sql/docs/mysql/optimize-costs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping sources:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 22/23 [01:29<00:03,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping https://www.cloudability.com/blog/cloud-cost-optimization/: 404 Client Error: Not Found for url: https://www.apptio.com/products/cloudability/blog/cloud-cost-optimization/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping sources: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23/23 [01:33<00:00,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Total documents scraped: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def scrape_content(url):\n",
    "    \"\"\"Scrape and clean content from a URL\"\"\"\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
    "        response = requests.get(url, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Remove script, style, nav, footer elements\n",
    "        for element in soup(['script', 'style', 'nav', 'footer', 'header']):\n",
    "            element.decompose()\n",
    "        \n",
    "        # Extract text from main content areas\n",
    "        text = soup.get_text(separator='\\n', strip=True)\n",
    "        \n",
    "        # Clean up extra whitespace\n",
    "        lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "        text = '\\n'.join(lines)\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Scrape all sources\n",
    "raw_documents = []\n",
    "for source in tqdm(SOURCES, desc=\"Scraping sources\"):\n",
    "    content = scrape_content(source['url'])\n",
    "    if content:\n",
    "        raw_documents.append({\n",
    "            'content': content,\n",
    "            'metadata': source\n",
    "        })\n",
    "        print(f\"Scraped {source['name']}: {len(content)} characters\")\n",
    "    time.sleep(1)  # Be polite\n",
    "\n",
    "print(f\"\\nüìÑ Total documents scraped: {len(raw_documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chunk Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÇÔ∏è Created 294 chunks\n",
      "üìä Average chunk size: 1506 characters\n"
     ]
    }
   ],
   "source": [
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE * 4,  # Approx 4 chars per token\n",
    "    chunk_overlap=CHUNK_OVERLAP * 4,\n",
    "    length_function=len,\n",
    "    separators=['\\n\\n', '\\n', '. ', ' ', '']\n",
    ")\n",
    "\n",
    "# Create Document objects and chunk them\n",
    "all_chunks = []\n",
    "chunk_id = 1\n",
    "\n",
    "for doc in raw_documents:\n",
    "    # Split text into chunks\n",
    "    chunks = text_splitter.split_text(doc['content'])\n",
    "    \n",
    "    # Create Document objects with metadata\n",
    "    for chunk in chunks:\n",
    "        all_chunks.append(\n",
    "            Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\n",
    "                    'id': chunk_id,\n",
    "                    'source': doc['metadata']['name'],\n",
    "                    'url': doc['metadata']['url'],\n",
    "                    'provider': doc['metadata']['provider']\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        chunk_id += 1\n",
    "\n",
    "print(f\"‚úÇÔ∏è Created {len(all_chunks)} chunks\")\n",
    "print(f\"üìä Average chunk size: {sum(len(c.page_content) for c in all_chunks) // len(all_chunks)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Embeddings & Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vector store initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenAI embeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=EMBEDDING_MODEL,\n",
    "    openai_api_key=os.getenv('OPENAI_API_KEY')\n",
    ")\n",
    "\n",
    "# Initialize PGVector store\n",
    "vector_store = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection=CONNECTION,\n",
    "    use_jsonb=True,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Vector store initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Add Documents to Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding to vector DB: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:33<00:00,  5.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Added 294 chunks to vector database\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Add documents in batches to avoid rate limits\n",
    "BATCH_SIZE = 50\n",
    "total_added = 0\n",
    "\n",
    "for i in tqdm(range(0, len(all_chunks), BATCH_SIZE), desc=\"Adding to vector DB\"):\n",
    "    batch = all_chunks[i:i+BATCH_SIZE]\n",
    "    ids = [str(doc.metadata['id']) for doc in batch]\n",
    "    \n",
    "    vector_store.add_documents(batch, ids=ids)\n",
    "    total_added += len(batch)\n",
    "    \n",
    "    # Rate limiting for OpenAI API\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"\\n‚úÖ Added {total_added} chunks to vector database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üìä INGESTION STATISTICS\n",
      "==================================================\n",
      "Total Sources: 23\n",
      "Documents Scraped: 21\n",
      "Total Chunks: 294\n",
      "Embedding Model: text-embedding-3-small\n",
      "Embedding Dimensions: 1536\n",
      "Average Chunk Size: 1506 characters\n",
      "\n",
      "Provider Breakdown:\n",
      "  AWS: 93 chunks\n",
      "  Azure: 78 chunks\n",
      "  GCP: 123 chunks\n",
      "==================================================\n",
      "\n",
      "‚úÖ Statistics saved to data/ingestion_stats.json\n"
     ]
    }
   ],
   "source": [
    "# Calculate statistics\n",
    "provider_counts = {}\n",
    "for chunk in all_chunks:\n",
    "    provider = chunk.metadata['provider']\n",
    "    provider_counts[provider] = provider_counts.get(provider, 0) + 1\n",
    "\n",
    "stats = {\n",
    "    'total_sources': len(SOURCES),\n",
    "    'total_documents_scraped': len(raw_documents),\n",
    "    'total_chunks': len(all_chunks),\n",
    "    'embedding_model': EMBEDDING_MODEL,\n",
    "    'embedding_dimensions': 1536,  # text-embedding-3-small dimension\n",
    "    'chunk_size_config': CHUNK_SIZE,\n",
    "    'chunk_overlap': CHUNK_OVERLAP,\n",
    "    'avg_chunk_size_chars': sum(len(c.page_content) for c in all_chunks) // len(all_chunks),\n",
    "    'provider_breakdown': provider_counts,\n",
    "    'vector_db': 'pgvector',\n",
    "    'collection_name': COLLECTION_NAME\n",
    "}\n",
    "\n",
    "# Save statistics\n",
    "os.makedirs('data', exist_ok=True)\n",
    "with open('data/ingestion_stats.json', 'w') as f:\n",
    "    json.dump(stats, f, indent=2)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä INGESTION STATISTICS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total Sources: {stats['total_sources']}\")\n",
    "print(f\"Documents Scraped: {stats['total_documents_scraped']}\")\n",
    "print(f\"Total Chunks: {stats['total_chunks']}\")\n",
    "print(f\"Embedding Model: {stats['embedding_model']}\")\n",
    "print(f\"Embedding Dimensions: {stats['embedding_dimensions']}\")\n",
    "print(f\"Average Chunk Size: {stats['avg_chunk_size_chars']} characters\")\n",
    "print(f\"\\nProvider Breakdown:\")\n",
    "for provider, count in provider_counts.items():\n",
    "    print(f\"  {provider}: {count} chunks\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\n‚úÖ Statistics saved to data/ingestion_stats.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Query (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Quick test to verify vector store works\n",
    "# test_query = \"How to reduce S3 storage costs?\"\n",
    "# results = vector_store.similarity_search(test_query, k=3)\n",
    "\n",
    "# print(f\"üîç Test Query: '{test_query}'\\n\")\n",
    "# for i, doc in enumerate(results, 1):\n",
    "#     print(f\"Result {i}:\")\n",
    "#     print(f\"Source: {doc.metadata['source']}\")\n",
    "#     print(f\"Content: {doc.page_content[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell: Clear Existing Data from Vector Database\n",
    "\n",
    "# from langchain_postgres import PGVector\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# import os\n",
    "\n",
    "# # Initialize connection\n",
    "# embeddings = OpenAIEmbeddings(\n",
    "#     model=EMBEDDING_MODEL,\n",
    "#     openai_api_key=os.getenv('OPENAI_API_KEY')\n",
    "# )\n",
    "\n",
    "# # Connect to vector store\n",
    "# vector_store = PGVector(\n",
    "#     embeddings=embeddings,\n",
    "#     collection_name=COLLECTION_NAME,\n",
    "#     connection=CONNECTION,\n",
    "#     use_jsonb=True,\n",
    "# )\n",
    "\n",
    "# # Delete the collection (drops the table)\n",
    "# try:\n",
    "#     vector_store.delete_collection()\n",
    "#     print(\"‚úÖ Successfully deleted existing collection and all data\")\n",
    "#     print(f\"   Collection '{COLLECTION_NAME}' has been removed\")\n",
    "# except Exception as e:\n",
    "#     print(f\"‚ö†Ô∏è Note: {str(e)}\")\n",
    "#     print(\"   (This is OK if collection didn't exist)\")\n",
    "\n",
    "# print(\"\\nüîÑ Ready to re-ingest with new sources!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
