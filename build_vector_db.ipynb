{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A: Data Ingestion & Vector Database\n",
    "Build vector database from cloud cost optimization sources using OpenAI embeddings and PGVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_postgres import PGVector\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ Imports loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Using embedding model: text-embedding-3-small\n",
      "üíæ Collection name: cloud_cost_optimization\n"
     ]
    }
   ],
   "source": [
    "# OpenAI Configuration\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "\n",
    "# PGVector Configuration\n",
    "CONNECTION = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"\n",
    "COLLECTION_NAME = \"cloud_cost_optimization\"\n",
    "\n",
    "# Chunking Configuration\n",
    "CHUNK_SIZE = 400  # tokens (roughly 1600 characters)\n",
    "CHUNK_OVERLAP = 50  # tokens\n",
    "\n",
    "print(f\"üìù Using embedding model: {EMBEDDING_MODEL}\")\n",
    "print(f\"üíæ Collection name: {COLLECTION_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Configured 23 data sources\n",
      "  - AWS Cost Optimization Pillar (AWS)\n",
      "  - AWS S3 Storage Classes (AWS)\n",
      "  - AWS EC2 Pricing (AWS)\n",
      "  - AWS Cost Optimization Blog (AWS)\n",
      "  - Azure Cost Management Best Practices (Azure)\n",
      "  - Azure Advisor Cost Recommendations (Azure)\n",
      "  - Azure Storage Cost Optimization (Azure)\n",
      "  - GCP Cost Optimization Best Practices (GCP)\n",
      "  - GCP Cloud Storage Cost Optimization (GCP)\n",
      "  - GCP Compute Engine Pricing (GCP)\n",
      "  - AWS Reserved Instances Guide (AWS)\n",
      "  - AWS Spot Instances Best Practices (AWS)\n",
      "  - Azure Reserved VM Instances (Azure)\n",
      "  - Azure Spot Virtual Machines (Azure)\n",
      "  - GCP Committed Use Discounts (GCP)\n",
      "  - GCP Preemptible VM Instances (GCP)\n",
      "  - AWS Lambda Cost Optimization (AWS)\n",
      "  - Azure Functions Cost Optimization (Azure)\n",
      "  - GCP Cloud Functions Pricing (GCP)\n",
      "  - AWS RDS Cost Optimization (AWS)\n",
      "  - Azure SQL Database Cost Optimization (Azure)\n",
      "  - GCP Cloud SQL Cost Optimization (GCP)\n",
      "  - Cloud Cost Optimization Guide - Cloudability (Third-party)\n"
     ]
    }
   ],
   "source": [
    "# Load sources from YAML configuration for better management\n",
    "import yaml\n",
    "\n",
    "with open('data/sources.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    SOURCES = config['sources']\n",
    "\n",
    "print(f\"üìö Configured {len(SOURCES)} data sources\")\n",
    "for source in SOURCES:\n",
    "    print(f\"  - {source['name']} ({source['provider']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Scrape and Clean Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping sources:   0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped AWS Cost Optimization Pillar: 2659 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping sources:   4%|‚ñç         | 1/23 [00:01<00:34,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped AWS S3 Storage Classes: 27231 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping sources:   9%|‚ñä         | 2/23 [00:03<00:34,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped AWS EC2 Pricing: 3109 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping sources:   9%|‚ñä         | 2/23 [00:04<00:48,  2.29s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     31\u001b[39m         raw_documents.append({\n\u001b[32m     32\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m: content,\n\u001b[32m     33\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m'\u001b[39m: source\n\u001b[32m     34\u001b[39m         })\n\u001b[32m     35\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mScraped \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource[\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(content)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m characters\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Be polite\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìÑ Total documents scraped: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(raw_documents)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def scrape_content(url):\n",
    "    \"\"\"Scrape and clean content from a URL\"\"\"\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
    "        response = requests.get(url, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Remove script, style, nav, footer elements\n",
    "        for element in soup(['script', 'style', 'nav', 'footer', 'header']):\n",
    "            element.decompose()\n",
    "        \n",
    "        # Extract text from main content areas\n",
    "        text = soup.get_text(separator='\\n', strip=True)\n",
    "        \n",
    "        # Clean up extra whitespace\n",
    "        lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "        text = '\\n'.join(lines)\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Scrape all sources\n",
    "raw_documents = []\n",
    "for source in tqdm(SOURCES, desc=\"Scraping sources\"):\n",
    "    content = scrape_content(source['url'])\n",
    "    if content:\n",
    "        raw_documents.append({\n",
    "            'content': content,\n",
    "            'metadata': source\n",
    "        })\n",
    "        print(f\"Scraped {source['name']}: {len(content)} characters\")\n",
    "    time.sleep(1)  # Be polite\n",
    "\n",
    "print(f\"\\nüìÑ Total documents scraped: {len(raw_documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chunk Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÇÔ∏è Created 294 chunks\n",
      "üìä Average chunk size: 1506 characters\n"
     ]
    }
   ],
   "source": [
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE * 4,  # Approx 4 chars per token\n",
    "    chunk_overlap=CHUNK_OVERLAP * 4,\n",
    "    length_function=len,\n",
    "    separators=['\\n\\n', '\\n', '. ', ' ', '']\n",
    ")\n",
    "\n",
    "# Create Document objects and chunk them\n",
    "all_chunks = []\n",
    "chunk_id = 1\n",
    "\n",
    "for doc in raw_documents:\n",
    "    # Split text into chunks\n",
    "    chunks = text_splitter.split_text(doc['content'])\n",
    "    \n",
    "    # Create Document objects with metadata\n",
    "    for chunk in chunks:\n",
    "        all_chunks.append(\n",
    "            Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\n",
    "                    'id': chunk_id,\n",
    "                    'source': doc['metadata']['name'],\n",
    "                    'url': doc['metadata']['url'],\n",
    "                    'provider': doc['metadata']['provider']\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        chunk_id += 1\n",
    "\n",
    "print(f\"‚úÇÔ∏è Created {len(all_chunks)} chunks\")\n",
    "print(f\"üìä Average chunk size: {sum(len(c.page_content) for c in all_chunks) // len(all_chunks)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Embeddings & Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vector store initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenAI embeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=EMBEDDING_MODEL,\n",
    "    openai_api_key=os.getenv('OPENAI_API_KEY')\n",
    ")\n",
    "\n",
    "# Initialize PGVector store\n",
    "vector_store = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection=CONNECTION,\n",
    "    use_jsonb=True,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Vector store initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Add Documents to Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding to vector DB: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:33<00:00,  5.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Added 294 chunks to vector database\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Add documents in batches to avoid rate limits\n",
    "BATCH_SIZE = 50\n",
    "total_added = 0\n",
    "\n",
    "for i in tqdm(range(0, len(all_chunks), BATCH_SIZE), desc=\"Adding to vector DB\"):\n",
    "    batch = all_chunks[i:i+BATCH_SIZE]\n",
    "    ids = [str(doc.metadata['id']) for doc in batch]\n",
    "    \n",
    "    vector_store.add_documents(batch, ids=ids)\n",
    "    total_added += len(batch)\n",
    "    \n",
    "    # Rate limiting for OpenAI API\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"\\n‚úÖ Added {total_added} chunks to vector database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üìä INGESTION STATISTICS\n",
      "==================================================\n",
      "Total Sources: 23\n",
      "Documents Scraped: 21\n",
      "Total Chunks: 294\n",
      "Embedding Model: text-embedding-3-small\n",
      "Embedding Dimensions: 1536\n",
      "Average Chunk Size: 1506 characters\n",
      "\n",
      "Provider Breakdown:\n",
      "  AWS: 93 chunks\n",
      "  Azure: 78 chunks\n",
      "  GCP: 123 chunks\n",
      "==================================================\n",
      "\n",
      "‚úÖ Statistics saved to data/ingestion_stats.json\n"
     ]
    }
   ],
   "source": [
    "# Calculate statistics\n",
    "provider_counts = {}\n",
    "for chunk in all_chunks:\n",
    "    provider = chunk.metadata['provider']\n",
    "    provider_counts[provider] = provider_counts.get(provider, 0) + 1\n",
    "\n",
    "stats = {\n",
    "    'total_sources': len(SOURCES),\n",
    "    'total_documents_scraped': len(raw_documents),\n",
    "    'total_chunks': len(all_chunks),\n",
    "    'embedding_model': EMBEDDING_MODEL,\n",
    "    'embedding_dimensions': 1536,  # text-embedding-3-small dimension\n",
    "    'chunk_size_config': CHUNK_SIZE,\n",
    "    'chunk_overlap': CHUNK_OVERLAP,\n",
    "    'avg_chunk_size_chars': sum(len(c.page_content) for c in all_chunks) // len(all_chunks),\n",
    "    'provider_breakdown': provider_counts,\n",
    "    'vector_db': 'pgvector',\n",
    "    'collection_name': COLLECTION_NAME\n",
    "}\n",
    "\n",
    "# Save statistics\n",
    "os.makedirs('data', exist_ok=True)\n",
    "with open('data/ingestion_stats.json', 'w') as f:\n",
    "    json.dump(stats, f, indent=2)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä INGESTION STATISTICS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total Sources: {stats['total_sources']}\")\n",
    "print(f\"Documents Scraped: {stats['total_documents_scraped']}\")\n",
    "print(f\"Total Chunks: {stats['total_chunks']}\")\n",
    "print(f\"Embedding Model: {stats['embedding_model']}\")\n",
    "print(f\"Embedding Dimensions: {stats['embedding_dimensions']}\")\n",
    "print(f\"Average Chunk Size: {stats['avg_chunk_size_chars']} characters\")\n",
    "print(f\"\\nProvider Breakdown:\")\n",
    "for provider, count in provider_counts.items():\n",
    "    print(f\"  {provider}: {count} chunks\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\n‚úÖ Statistics saved to data/ingestion_stats.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Query (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Quick test to verify vector store works\n",
    "# test_query = \"How to reduce S3 storage costs?\"\n",
    "# results = vector_store.similarity_search(test_query, k=3)\n",
    "\n",
    "# print(f\"üîç Test Query: '{test_query}'\\n\")\n",
    "# for i, doc in enumerate(results, 1):\n",
    "#     print(f\"Result {i}:\")\n",
    "#     print(f\"Source: {doc.metadata['source']}\")\n",
    "#     print(f\"Content: {doc.page_content[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell: Clear Existing Data from Vector Database\n",
    "\n",
    "# from langchain_postgres import PGVector\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# import os\n",
    "\n",
    "# # Initialize connection\n",
    "# embeddings = OpenAIEmbeddings(\n",
    "#     model=EMBEDDING_MODEL,\n",
    "#     openai_api_key=os.getenv('OPENAI_API_KEY')\n",
    "# )\n",
    "\n",
    "# # Connect to vector store\n",
    "# vector_store = PGVector(\n",
    "#     embeddings=embeddings,\n",
    "#     collection_name=COLLECTION_NAME,\n",
    "#     connection=CONNECTION,\n",
    "#     use_jsonb=True,\n",
    "# )\n",
    "\n",
    "# # Delete the collection (drops the table)\n",
    "# try:\n",
    "#     vector_store.delete_collection()\n",
    "#     print(\"‚úÖ Successfully deleted existing collection and all data\")\n",
    "#     print(f\"   Collection '{COLLECTION_NAME}' has been removed\")\n",
    "# except Exception as e:\n",
    "#     print(f\"‚ö†Ô∏è Note: {str(e)}\")\n",
    "#     print(\"   (This is OK if collection didn't exist)\")\n",
    "\n",
    "# print(\"\\nüîÑ Ready to re-ingest with new sources!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Add New Sources to Existing Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Ready to add 1 new sources:\n",
      "  - Databricks Spark Delta Lake Optimization Guide (Databricks)\n"
     ]
    }
   ],
   "source": [
    "# Add new sources to existing vector database without recreating it\n",
    "# This cell allows you to incrementally add more documents\n",
    "\n",
    "# Define new sources to add\n",
    "NEW_SOURCES = [\n",
    "    {\n",
    "        'name': 'Databricks Spark Delta Lake Optimization Guide',\n",
    "        'url': 'https://www.databricks.com/discover/pages/optimize-data-workloads-guide',\n",
    "        'provider': 'Databricks'\n",
    "    },\n",
    "    # Add more sources here as needed\n",
    "    # {\n",
    "    #     'name': 'Another Source',\n",
    "    #     'url': 'https://example.com/another-guide',\n",
    "    #     'provider': 'Provider Name'\n",
    "    # }\n",
    "]\n",
    "\n",
    "print(f\"üìö Ready to add {len(NEW_SOURCES)} new sources:\")\n",
    "for source in NEW_SOURCES:\n",
    "    print(f\"  - {source['name']} ({source['provider']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Scraping new sources...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping new sources:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped Databricks Spark Delta Lake Optimization Guide: 52909 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping new sources: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:03<00:00,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ New documents scraped: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def scrape_content(url):\n",
    "    \"\"\"Scrape and clean content from a URL\"\"\"\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
    "        response = requests.get(url, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Remove script, style, nav, footer elements\n",
    "        for element in soup(['script', 'style', 'nav', 'footer', 'header']):\n",
    "            element.decompose()\n",
    "        \n",
    "        # Extract text from main content areas\n",
    "        text = soup.get_text(separator='\\n', strip=True)\n",
    "        \n",
    "        # Clean up extra whitespace\n",
    "        lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "        text = '\\n'.join(lines)\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "# Scrape new sources\n",
    "print(\"\\nüîÑ Scraping new sources...\")\n",
    "new_raw_documents = []\n",
    "\n",
    "for source in tqdm(NEW_SOURCES, desc=\"Scraping new sources\"):\n",
    "    content = scrape_content(source['url'])\n",
    "    if content:\n",
    "        new_raw_documents.append({\n",
    "            'content': content,\n",
    "            'metadata': source\n",
    "        })\n",
    "        print(f\"Scraped {source['name']}: {len(content)} characters\")\n",
    "    time.sleep(1)  # Be polite\n",
    "\n",
    "print(f\"\\nüìÑ New documents scraped: {len(new_raw_documents)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÇÔ∏è Chunking new documents...\n",
      "Starting new chunk IDs from: 1000\n",
      "‚úÇÔ∏è Created 38 new chunks\n",
      "üìä Average new chunk size: 1491 characters\n"
     ]
    }
   ],
   "source": [
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE * 4,  # Approx 4 chars per token\n",
    "    chunk_overlap=CHUNK_OVERLAP * 4,\n",
    "    length_function=len,\n",
    "    separators=['\\n\\n', '\\n', '. ', ' ', '']\n",
    ")\n",
    "\n",
    "#  Chunk new documents\n",
    "print(\"\\n‚úÇÔ∏è Chunking new documents...\")\n",
    "\n",
    "# Get the current highest chunk ID to continue numbering\n",
    "try:\n",
    "    # Query existing vector store to get max chunk ID\n",
    "    existing_docs = vector_store.similarity_search(\"\", k=1000)  # Get many docs to find max ID\n",
    "    if existing_docs:\n",
    "        max_id = max(int(doc.metadata.get('id', 0)) for doc in existing_docs)\n",
    "        next_chunk_id = max_id + 1\n",
    "    else:\n",
    "        next_chunk_id = 1\n",
    "except:\n",
    "    # If we can't determine max ID, start from a high number to avoid conflicts\n",
    "    next_chunk_id = 1000\n",
    "\n",
    "print(f\"Starting new chunk IDs from: {next_chunk_id}\")\n",
    "\n",
    "# Create new chunks\n",
    "new_chunks = []\n",
    "for doc in new_raw_documents:\n",
    "    # Split text into chunks\n",
    "    chunks = text_splitter.split_text(doc['content'])\n",
    "    \n",
    "    # Create Document objects with metadata\n",
    "    for chunk in chunks:\n",
    "        new_chunks.append(\n",
    "            Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\n",
    "                    'id': next_chunk_id,\n",
    "                    'source': doc['metadata']['name'],\n",
    "                    'url': doc['metadata']['url'],\n",
    "                    'provider': doc['metadata']['provider']\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        next_chunk_id += 1\n",
    "\n",
    "print(f\"‚úÇÔ∏è Created {len(new_chunks)} new chunks\")\n",
    "if new_chunks:\n",
    "    print(f\"üìä Average new chunk size: {sum(len(c.page_content) for c in new_chunks) // len(new_chunks)} characters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vector store initialized\n",
      "\n",
      "üíæ Adding 38 new chunks to existing vector database...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding new chunks to vector DB: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [01:05<00:00, 65.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Successfully added 38 new chunks to vector database\n",
      "üìä Vector database now contains approximately 332 total chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenAI embeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=EMBEDDING_MODEL,\n",
    "    openai_api_key=os.getenv('OPENAI_API_KEY')\n",
    ")\n",
    "\n",
    "# Initialize PGVector store\n",
    "vector_store = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection=CONNECTION,\n",
    "    use_jsonb=True,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Vector store initialized\")\n",
    "\n",
    "\n",
    "# Add new documents to existing vector store\n",
    "if new_chunks:\n",
    "    print(f\"\\nüíæ Adding {len(new_chunks)} new chunks to existing vector database...\")\n",
    "    \n",
    "    # Add documents in batches to avoid rate limits\n",
    "    BATCH_SIZE = 50\n",
    "    total_added = 0\n",
    "    \n",
    "    for i in tqdm(range(0, len(new_chunks), BATCH_SIZE), desc=\"Adding new chunks to vector DB\"):\n",
    "        batch = new_chunks[i:i+BATCH_SIZE]\n",
    "        ids = [str(doc.metadata['id']) for doc in batch]\n",
    "        \n",
    "        vector_store.add_documents(batch, ids=ids)\n",
    "        total_added += len(batch)\n",
    "        \n",
    "        # Rate limiting for OpenAI API\n",
    "        time.sleep(1)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Successfully added {total_added} new chunks to vector database\")\n",
    "    print(f\"üìä Vector database now contains approximately {294 + total_added} total chunks\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No new chunks to add\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Testing new additions with a sample query...\n",
      "\n",
      "Test Query: 'How to optimize Databricks Spark workloads for better performance?'\n",
      "\n",
      "Top results:\n",
      "\n",
      "1. Source: Databricks Spark Delta Lake Optimization Guide\n",
      "   Provider: Databricks\n",
      "   Content: spark.executor.defaultJavaOptions\n",
      "and\n",
      "spark.executor.extraJavaOptions\n",
      "to\n",
      "-XX:+UseG1GC\n",
      "value in Spark config section under Advance cluster options\n",
      "Plea...\n",
      "\n",
      "2. Source: Databricks Spark Delta Lake Optimization Guide\n",
      "   Provider: Databricks\n",
      "   Content: Delta Live Tables might be a better fit for such workloads, as DLT determines the complete DAG of the pipeline and then goes about running it in the m...\n",
      "\n",
      "3. Source: Databricks Spark Delta Lake Optimization Guide\n",
      "   Provider: Databricks\n",
      "   Content: Comprehensive Guide to Optimize Data Workloads | Databricks\n",
      "Skip to main content\n",
      "eBook\n",
      "Comprehensive Guide to Optimize Databricks, Spark and Delta Lak...\n",
      "\n",
      "4. Source: Databricks Spark Delta Lake Optimization Guide\n",
      "   Provider: Databricks\n",
      "   Content: Accelerated queries that process a significant amount of data (> 100GB) and include aggregations and joins\n",
      "Faster performance when data is accessed re...\n",
      "\n",
      "5. Source: Databricks Spark Delta Lake Optimization Guide\n",
      "   Provider: Databricks\n",
      "   Content: flavor, offering you access to an instant compute.\n",
      "Autoscaling\n",
      "Databricks provides a unique feature of\n",
      "cluster autoscaling\n",
      ". Here are some guidelines ...\n",
      "\n",
      "‚úÖ Found 5 Databricks-related results in top 5!\n",
      "New sources have been successfully added to the vector database.\n"
     ]
    }
   ],
   "source": [
    "# Test the new additions with a query\n",
    "if new_chunks:\n",
    "    print(\"\\nüîç Testing new additions with a sample query...\")\n",
    "    \n",
    "    # Test query related to Databricks/Spark optimization\n",
    "    test_query = \"How to optimize Databricks Spark workloads for better performance?\"\n",
    "    results = vector_store.similarity_search(test_query, k=5)\n",
    "    \n",
    "    print(f\"\\nTest Query: '{test_query}'\\n\")\n",
    "    print(\"Top results:\")\n",
    "    \n",
    "    databricks_results = 0\n",
    "    for i, doc in enumerate(results, 1):\n",
    "        provider = doc.metadata.get('provider', 'Unknown')\n",
    "        source = doc.metadata.get('source', 'Unknown')\n",
    "        \n",
    "        if 'databricks' in provider.lower() or 'databricks' in source.lower():\n",
    "            databricks_results += 1\n",
    "            \n",
    "        print(f\"\\n{i}. Source: {source}\")\n",
    "        print(f\"   Provider: {provider}\")\n",
    "        print(f\"   Content: {doc.page_content[:150]}...\")\n",
    "    \n",
    "    if databricks_results > 0:\n",
    "        print(f\"\\n‚úÖ Found {databricks_results} Databricks-related results in top 5!\")\n",
    "        print(\"New sources have been successfully added to the vector database.\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è No Databricks results in top 5. This might be normal if the query doesn't match well.\")\n",
    "        print(\"The new sources have been added, but try different queries to verify.\")\n",
    "else:\n",
    "    print(\"\\n‚è≠Ô∏è Skipping test - no new chunks were added\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Updating statistics...\n",
      "‚úÖ Statistics updated and saved\n",
      "üìà New totals: 24 sources, 332 chunks\n",
      "üÜï Added providers: ['Databricks']\n"
     ]
    }
   ],
   "source": [
    "# Update statistics with new additions\n",
    "if new_chunks:\n",
    "    print(\"\\nüìä Updating statistics...\")\n",
    "    \n",
    "    # Load existing stats\n",
    "    try:\n",
    "        with open('data/ingestion_stats.json', 'r') as f:\n",
    "            stats = json.load(f)\n",
    "    except:\n",
    "        stats = {}\n",
    "    \n",
    "    # Update with new data\n",
    "    new_provider_counts = {}\n",
    "    for chunk in new_chunks:\n",
    "        provider = chunk.metadata['provider']\n",
    "        new_provider_counts[provider] = new_provider_counts.get(provider, 0) + 1\n",
    "    \n",
    "    # Merge provider counts\n",
    "    if 'provider_breakdown' not in stats:\n",
    "        stats['provider_breakdown'] = {}\n",
    "    \n",
    "    for provider, count in new_provider_counts.items():\n",
    "        if provider in stats['provider_breakdown']:\n",
    "            stats['provider_breakdown'][provider] += count\n",
    "        else:\n",
    "            stats['provider_breakdown'][provider] = count\n",
    "    \n",
    "    # Update totals\n",
    "    stats['total_chunks'] = stats.get('total_chunks', 294) + len(new_chunks)\n",
    "    stats['total_sources'] = stats.get('total_sources', 23) + len(NEW_SOURCES)\n",
    "    stats['last_update'] = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # Save updated stats\n",
    "    with open('data/ingestion_stats.json', 'w') as f:\n",
    "        json.dump(stats, f, indent=2)\n",
    "    \n",
    "    print(\"‚úÖ Statistics updated and saved\")\n",
    "    print(f\"üìà New totals: {stats['total_sources']} sources, {stats['total_chunks']} chunks\")\n",
    "    print(f\"üÜï Added providers: {list(new_provider_counts.keys())}\")\n",
    "else:\n",
    "    print(\"\\n‚è≠Ô∏è No statistics to update\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Injest PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"/Users/sarfarazahmed/Desktop/optimization-rag-system/data/data/dokumen.pub_finops-handbook-for-microsoft-azure-empowering-teams-to-optimize-their-azure-cloud-spend-with-finops-best-practices-9781801810166-1801810168-j-3887050.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "pages = []\n",
    "for doc in loader.lazy_load():\n",
    "    pages.append(doc)\n",
    "    # if len(pages) >= 10:\n",
    "    #     # do some paged operation, e.g.\n",
    "    #     # index.upsert(page)\n",
    "\n",
    "    #     pages = []\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs  = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 473 sub-documents.\n",
      "Total characters: 324613\n",
      "Total documents: 256\n",
      "Average characters per document: 1268\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")\n",
    "print(f\"Total characters: {sum(len(doc.page_content) for doc in docs)}\")\n",
    "print(f\"Total documents: {len(docs)}\")\n",
    "print(f\"Average characters per document: {sum(len(doc.page_content) for doc in docs) // len(docs) if docs else 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vector store initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenAI embeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=EMBEDDING_MODEL,\n",
    "    openai_api_key=os.getenv('OPENAI_API_KEY')\n",
    ")\n",
    "\n",
    "# Initialize PGVector store\n",
    "vector_store = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection=CONNECTION,\n",
    "    use_jsonb=True,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Vector store initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "print(document_ids[:3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
