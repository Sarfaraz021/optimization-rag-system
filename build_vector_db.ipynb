{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A: Data Ingestion & Vector Database\n",
    "Build vector database from cloud cost optimization sources using OpenAI embeddings and PGVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_postgres import PGVector\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ Imports loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI Configuration\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "\n",
    "# PGVector Configuration\n",
    "CONNECTION = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"\n",
    "COLLECTION_NAME = \"cloud_cost_optimization\"\n",
    "\n",
    "# Chunking Configuration\n",
    "CHUNK_SIZE = 400  # tokens (roughly 1600 characters)\n",
    "CHUNK_OVERLAP = 50  # tokens\n",
    "\n",
    "print(f\"üìù Using embedding model: {EMBEDDING_MODEL}\")\n",
    "print(f\"üíæ Collection name: {COLLECTION_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sources from YAML configuration for better management\n",
    "import yaml\n",
    "\n",
    "with open('data/sources.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    SOURCES = config['sources']\n",
    "\n",
    "print(f\"üìö Configured {len(SOURCES)} data sources\")\n",
    "for source in SOURCES:\n",
    "    print(f\"  - {source['name']} ({source['provider']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Scrape and Clean Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_content(url):\n",
    "    \"\"\"Scrape and clean content from a URL\"\"\"\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
    "        response = requests.get(url, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Remove script, style, nav, footer elements\n",
    "        for element in soup(['script', 'style', 'nav', 'footer', 'header']):\n",
    "            element.decompose()\n",
    "        \n",
    "        # Extract text from main content areas\n",
    "        text = soup.get_text(separator='\\n', strip=True)\n",
    "        \n",
    "        # Clean up extra whitespace\n",
    "        lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "        text = '\\n'.join(lines)\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Scrape all sources\n",
    "raw_documents = []\n",
    "for source in tqdm(SOURCES, desc=\"Scraping sources\"):\n",
    "    content = scrape_content(source['url'])\n",
    "    if content:\n",
    "        raw_documents.append({\n",
    "            'content': content,\n",
    "            'metadata': source\n",
    "        })\n",
    "        print(f\"Scraped {source['name']}: {len(content)} characters\")\n",
    "    time.sleep(1)  # Be polite\n",
    "\n",
    "print(f\"\\nüìÑ Total documents scraped: {len(raw_documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chunk Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE * 4,  # Approx 4 chars per token\n",
    "    chunk_overlap=CHUNK_OVERLAP * 4,\n",
    "    length_function=len,\n",
    "    separators=['\\n\\n', '\\n', '. ', ' ', '']\n",
    ")\n",
    "\n",
    "# Create Document objects and chunk them\n",
    "all_chunks = []\n",
    "chunk_id = 1\n",
    "\n",
    "for doc in raw_documents:\n",
    "    # Split text into chunks\n",
    "    chunks = text_splitter.split_text(doc['content'])\n",
    "    \n",
    "    # Create Document objects with metadata\n",
    "    for chunk in chunks:\n",
    "        all_chunks.append(\n",
    "            Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\n",
    "                    'id': chunk_id,\n",
    "                    'source': doc['metadata']['name'],\n",
    "                    'url': doc['metadata']['url'],\n",
    "                    'provider': doc['metadata']['provider']\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        chunk_id += 1\n",
    "\n",
    "print(f\"‚úÇÔ∏è Created {len(all_chunks)} chunks\")\n",
    "print(f\"üìä Average chunk size: {sum(len(c.page_content) for c in all_chunks) // len(all_chunks)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Embeddings & Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI embeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=EMBEDDING_MODEL,\n",
    "    openai_api_key=os.getenv('OPENAI_API_KEY')\n",
    ")\n",
    "\n",
    "# Initialize PGVector store\n",
    "vector_store = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection=CONNECTION,\n",
    "    use_jsonb=True,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Vector store initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Add Documents to Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add documents in batches to avoid rate limits\n",
    "BATCH_SIZE = 50\n",
    "total_added = 0\n",
    "\n",
    "for i in tqdm(range(0, len(all_chunks), BATCH_SIZE), desc=\"Adding to vector DB\"):\n",
    "    batch = all_chunks[i:i+BATCH_SIZE]\n",
    "    ids = [str(doc.metadata['id']) for doc in batch]\n",
    "    \n",
    "    vector_store.add_documents(batch, ids=ids)\n",
    "    total_added += len(batch)\n",
    "    \n",
    "    # Rate limiting for OpenAI API\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"\\n‚úÖ Added {total_added} chunks to vector database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics\n",
    "provider_counts = {}\n",
    "for chunk in all_chunks:\n",
    "    provider = chunk.metadata['provider']\n",
    "    provider_counts[provider] = provider_counts.get(provider, 0) + 1\n",
    "\n",
    "stats = {\n",
    "    'total_sources': len(SOURCES),\n",
    "    'total_documents_scraped': len(raw_documents),\n",
    "    'total_chunks': len(all_chunks),\n",
    "    'embedding_model': EMBEDDING_MODEL,\n",
    "    'embedding_dimensions': 1536,  # text-embedding-3-small dimension\n",
    "    'chunk_size_config': CHUNK_SIZE,\n",
    "    'chunk_overlap': CHUNK_OVERLAP,\n",
    "    'avg_chunk_size_chars': sum(len(c.page_content) for c in all_chunks) // len(all_chunks),\n",
    "    'provider_breakdown': provider_counts,\n",
    "    'vector_db': 'pgvector',\n",
    "    'collection_name': COLLECTION_NAME\n",
    "}\n",
    "\n",
    "# Save statistics\n",
    "os.makedirs('data', exist_ok=True)\n",
    "with open('data/ingestion_stats.json', 'w') as f:\n",
    "    json.dump(stats, f, indent=2)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä INGESTION STATISTICS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total Sources: {stats['total_sources']}\")\n",
    "print(f\"Documents Scraped: {stats['total_documents_scraped']}\")\n",
    "print(f\"Total Chunks: {stats['total_chunks']}\")\n",
    "print(f\"Embedding Model: {stats['embedding_model']}\")\n",
    "print(f\"Embedding Dimensions: {stats['embedding_dimensions']}\")\n",
    "print(f\"Average Chunk Size: {stats['avg_chunk_size_chars']} characters\")\n",
    "print(f\"\\nProvider Breakdown:\")\n",
    "for provider, count in provider_counts.items():\n",
    "    print(f\"  {provider}: {count} chunks\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\n‚úÖ Statistics saved to data/ingestion_stats.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Query (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test to verify vector store works\n",
    "test_query = \"How to reduce S3 storage costs?\"\n",
    "results = vector_store.similarity_search(test_query, k=3)\n",
    "\n",
    "print(f\"üîç Test Query: '{test_query}'\\n\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"Result {i}:\")\n",
    "    print(f\"Source: {doc.metadata['source']}\")\n",
    "    print(f\"Content: {doc.page_content[:200]}...\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
